"""Phase 4: Intent Validation (Semantic Layer).

This module validates that the commit message semantically matches
the actual infrastructure changes being made.
"""

from dataclasses import dataclass
from typing import Any, Optional


@dataclass
class IntentValidationResult:
    """Result of intent validation."""

    aligned: bool
    explanation: str
    action_required: Optional[str]
    confidence: float
    report: Optional[str] = None  # Managerial report from LLM

    def __str__(self) -> str:
        """String representation."""
        status = "âœ… ALIGNED" if self.aligned else "âŒ MISMATCH"
        result = (
            f"{status}\n"
            f"   Confidence: {self.confidence:.0%}\n"
            f"   Explanation: {self.explanation}"
        )
        if self.report:
            result += f"\n\nðŸ“‹ Managerial Report:\n{self.report}"
        return result


@dataclass
class ChangeImpactReport:
    """Comprehensive report for tech managers about infrastructure changes."""

    summary: str
    current_state: str
    proposed_changes: str
    system_impact: str
    risk_assessment: str
    recommendations: str
    safety_rating: str  # "SAFE", "CAUTION", "RISKY", "DANGEROUS"
    confidence_score: float

    def to_markdown(self) -> str:
        """Convert report to markdown format for easy reading."""
        return f"""## Infrastructure Change Impact Report

### Executive Summary
{self.summary}

### Current State
{self.current_state}

### Proposed Changes
{self.proposed_changes}

### System Impact
{self.system_impact}

### Risk Assessment
**Safety Rating: {self.safety_rating}** (Confidence: {self.confidence_score:.0%})

{self.risk_assessment}

### Recommendations
{self.recommendations}

---
*Generated by tf-gate AI Analysis*
"""


class IntentValidator:
    """Validates semantic alignment between commit message and changes."""

    # Keywords that indicate specific types of changes
    KEYWORD_PATTERNS = {
        "tag": ["tag", "tags", "label", "labels"],
        "create": ["create", "add", "new", "provision"],
        "delete": ["delete", "remove", "destroy", "cleanup"],
        "update": ["update", "modify", "change", "patch", "fix"],
        "security": ["security", "secure", "encrypt", "tls", "ssl", "auth"],
        "cost": ["cost", "pricing", "expensive", "cheap", "optimize"],
        "database": ["database", "db", "rds", "postgres", "mysql"],
        "iam": ["iam", "role", "policy", "permission"],
    }

    def __init__(self, use_llm: bool = False, llm_provider: Optional[str] = None):
        """Initialize intent validator.

        Args:
            use_llm: Whether to use LLM for validation (if available).
            llm_provider: LLM provider name ("ollama", "openai", etc.).
        """
        self.use_llm = use_llm
        self.llm_provider = llm_provider

    def validate(
        self,
        commit_message: str,
        resource_changes: list[dict[str, Any]],
        generate_report: bool = False,
    ) -> IntentValidationResult:
        """Validate intent alignment.

        Args:
            commit_message: Git commit message describing intent.
            resource_changes: List of resource changes from plan.
            generate_report: Whether to generate a detailed impact report.

        Returns:
            IntentValidationResult with alignment status and optional report.
        """
        # First try keyword-based validation
        keyword_result = self._keyword_based_validation(commit_message, resource_changes)

        # If LLM is enabled, use it for better validation and optionally generate report
        if self.use_llm:
            try:
                llm_result = self._llm_based_validation(commit_message, resource_changes)
                
                # Generate impact report if requested
                if generate_report:
                    try:
                        report = self.generate_impact_report(commit_message, resource_changes)
                        llm_result.report = report.to_markdown()
                    except Exception as e:
                        # Report generation failed, but validation succeeded
                        llm_result.report = f"âš ï¸ Report generation failed: {e}"
                
                return llm_result
            except Exception:
                # Fall back to keyword result if LLM fails
                pass

        return keyword_result

    def _keyword_based_validation(
        self,
        commit_message: str,
        resource_changes: list[dict[str, Any]],
    ) -> IntentValidationResult:
        """Validate using keyword matching.

        Args:
            commit_message: Git commit message.
            resource_changes: List of resource changes.

        Returns:
            IntentValidationResult.
        """
        commit_lower = commit_message.lower()

        # Analyze actual changes
        has_creations = any(
            "create" in r.get("change", {}).get("actions", []) for r in resource_changes
        )
        has_deletions = any(
            "delete" in r.get("change", {}).get("actions", []) for r in resource_changes
        )
        has_updates = any(
            "update" in r.get("change", {}).get("actions", []) for r in resource_changes
        )

        # Check for mismatches
        mismatches = []

        # Tag-related commit but destructive changes
        if any(kw in commit_lower for kw in self.KEYWORD_PATTERNS["tag"]):
            if has_deletions:
                mismatches.append("Commit mentions tags but resources are being deleted")
            elif has_creations:
                mismatches.append("Commit mentions tags but resources are being created")

        # Create-related commit but only updates/deletes
        if any(kw in commit_lower for kw in self.KEYWORD_PATTERNS["create"]):
            if not has_creations and (has_updates or has_deletions):
                mismatches.append(
                    "Commit mentions creating resources but plan only shows updates/deletions"
                )

        # Delete-related commit but only creates/updates
        if any(kw in commit_lower for kw in self.KEYWORD_PATTERNS["delete"]):
            if not has_deletions and (has_creations or has_updates):
                mismatches.append("Commit mentions deletions but plan only shows creations/updates")

        # Security-related commit but no IAM/security resource changes
        if any(kw in commit_lower for kw in self.KEYWORD_PATTERNS["security"]):
            security_resources = [
                r
                for r in resource_changes
                if any(x in r.get("type", "") for x in ["iam", "security", "kms", "tls"])
            ]
            if not security_resources:
                mismatches.append(
                    "Commit mentions security but no security-related resources are changing"
                )

        # Database-related commit but no database changes
        if any(kw in commit_lower for kw in self.KEYWORD_PATTERNS["database"]):
            db_resources = [
                r
                for r in resource_changes
                if any(x in r.get("type", "") for x in ["db", "rds", "postgres", "mysql"])
            ]
            if not db_resources:
                mismatches.append("Commit mentions database but no database resources are changing")

        # Calculate confidence
        if not mismatches:
            confidence = 0.9
            explanation = "Commit message aligns with planned changes"
            action_required = None
            aligned = True
        elif len(mismatches) == 1:
            confidence = 0.5
            explanation = mismatches[0]
            action_required = "Review changes to ensure they match stated intent"
            aligned = False
        else:
            confidence = 0.2
            explanation = f"Multiple mismatches detected: {'; '.join(mismatches)}"
            action_required = (
                "Significant mismatch between commit message and changes. Please verify."
            )
            aligned = False

        return IntentValidationResult(
            aligned=aligned,
            explanation=explanation,
            action_required=action_required,
            confidence=confidence,
        )

    def _llm_based_validation(
        self,
        commit_message: str,
        resource_changes: list[dict[str, Any]],
    ) -> IntentValidationResult:
        """Validate using LLM.

        Args:
            commit_message: Git commit message.
            resource_changes: List of resource changes.

        Returns:
            IntentValidationResult.

        Raises:
            NotImplementedError: If LLM provider not available.
        """
        # Build a summary of changes
        change_summary = self._summarize_changes(resource_changes)

        prompt = f"""Analyze the semantic alignment between a git commit message and Terraform infrastructure changes.

Git Commit Message: "{commit_message}"

Terraform Changes Summary:
{change_summary}

Analyze: Does the commit message semantically match the infrastructure changes?

Respond in this exact format:
ALIGNMENT: [ALIGNED|MISMATCH|UNCERTAIN]
CONFIDENCE: [0-100]
EXPLANATION: [2-3 sentence explanation]
ACTION: [None|Review required|Human confirmation needed]
"""

        if self.llm_provider == "ollama":
            return self._call_ollama(prompt)
        elif self.llm_provider == "openai":
            return self._call_openai(prompt)
        elif self.llm_provider == "lmstudio":
            return self._call_lmstudio(prompt)
        else:
            raise NotImplementedError(f"LLM provider '{self.llm_provider}' not implemented")

    def _summarize_changes(self, resource_changes: list[dict[str, Any]]) -> str:
        """Create a human-readable summary of changes.

        Args:
            resource_changes: List of resource changes.

        Returns:
            Summary string.
        """
        if not resource_changes:
            return "No changes"

        summary_parts = []

        for resource in resource_changes[:20]:  # Limit to 20 resources
            address = resource.get("address", "unknown")
            actions = resource.get("change", {}).get("actions", [])
            action_str = "/".join(actions)
            resource_type = resource.get("type", "unknown")

            summary_parts.append(f"  - {action_str}: {address} ({resource_type})")

        if len(resource_changes) > 20:
            summary_parts.append(f"  ... and {len(resource_changes) - 20} more resources")

        return "\n".join(summary_parts)

    def _call_ollama(self, prompt: str) -> IntentValidationResult:
        """Call Ollama LLM.

        Args:
            prompt: Prompt text.

        Returns:
            IntentValidationResult.
        """
        try:
            import ollama

            response = ollama.generate(
                model="llama3",
                prompt=prompt,
            )

            return self._parse_llm_response(response.get("response", ""))
        except ImportError as e:
            raise NotImplementedError("ollama package not installed") from e

    def _call_openai(self, prompt: str) -> IntentValidationResult:
        """Call OpenAI API.

        Args:
            prompt: Prompt text.

        Returns:
            IntentValidationResult.
        """
        try:
            import openai

            client = openai.OpenAI()
            response = client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[
                    {
                        "role": "system",
                        "content": "You are analyzing Terraform plan alignment with commit messages.",
                    },
                    {"role": "user", "content": prompt},
                ],
                temperature=0.1,
            )

            content = response.choices[0].message.content
            if content is None:
                content = "ALIGNMENT: MISMATCH\nCONFIDENCE: 50\nEXPLANATION: Could not parse LLM response\nACTION: Review required"
            return self._parse_llm_response(content)
        except ImportError as e:
            raise NotImplementedError("openai package not installed") from e

    def _call_lmstudio(self, prompt: str) -> IntentValidationResult:
        """Call LMStudio local LLM via OpenAI-compatible API.

        Args:
            prompt: Prompt text.

        Returns:
            IntentValidationResult.
        """
        try:
            import openai

            client = openai.OpenAI(
                base_url="http://localhost:1234/v1",
                api_key="not-needed"  # LMStudio doesn't require an API key
            )
            response = client.chat.completions.create(
                model="qwen2.5-coder-7b-instruct",
                messages=[
                    {
                        "role": "system",
                        "content": "You are analyzing Terraform plan alignment with commit messages.",
                    },
                    {"role": "user", "content": prompt},
                ],
                temperature=0.7,
                max_tokens=-1,
            )

            content = response.choices[0].message.content
            if content is None:
                content = "ALIGNMENT: MISMATCH\nCONFIDENCE: 50\nEXPLANATION: Could not parse LLM response\nACTION: Review required"
            return self._parse_llm_response(content)
        except ImportError as e:
            raise NotImplementedError("openai package not installed") from e

    def _parse_llm_response(self, response: str) -> IntentValidationResult:
        """Parse LLM response into IntentValidationResult.

        Args:
            response: Raw LLM response text.

        Returns:
            IntentValidationResult.
        """
        lines = response.strip().split("\n")

        aligned = False
        confidence = 0.5
        explanation = "Unable to parse LLM response"
        action_required = "Review required"

        for line in lines:
            if line.startswith("ALIGNMENT:"):
                value = line.split(":", 1)[1].strip().upper()
                aligned = value == "ALIGNED"
            elif line.startswith("CONFIDENCE:"):
                try:
                    confidence = int(line.split(":", 1)[1].strip()) / 100.0
                except (ValueError, IndexError):
                    pass
            elif line.startswith("EXPLANATION:"):
                explanation = line.split(":", 1)[1].strip()
            elif line.startswith("ACTION:"):
                action_value = line.split(":", 1)[1].strip()
                if action_value.lower() == "none":
                    action_required = None
                else:
                    action_required = action_value

        return IntentValidationResult(
            aligned=aligned,
            explanation=explanation,
            action_required=action_required,
            confidence=confidence,
        )

    def generate_impact_report(
        self,
        commit_message: str,
        resource_changes: list[dict[str, Any]],
    ) -> ChangeImpactReport:
        """Generate a comprehensive impact report for tech managers.

        This report provides a detailed analysis of infrastructure changes,
        their impact on the system, and safety recommendations.

        Args:
            commit_message: Git commit message describing the intent.
            resource_changes: List of resource changes from the plan.

        Returns:
            ChangeImpactReport with detailed analysis.
        """
        change_summary = self._summarize_changes(resource_changes)

        # Calculate statistics
        total_resources = len(resource_changes)
        create_count = sum(1 for r in resource_changes if "create" in r.get("change", {}).get("actions", []))
        delete_count = sum(1 for r in resource_changes if "delete" in r.get("change", {}).get("actions", []))
        update_count = sum(1 for r in resource_changes if "update" in r.get("change", {}).get("actions", []))
        replace_count = sum(1 for r in resource_changes if set(r.get("change", {}).get("actions", [])) == {"create", "delete"})

        # Identify critical resources
        critical_types = {"aws_db_instance", "aws_rds_cluster", "aws_kms_key", "aws_s3_bucket", 
                         "aws_dynamodb_table", "aws_elasticache_cluster"}
        critical_changes = [r for r in resource_changes if r.get("type", "") in critical_types]

        prompt = f"""You are an expert DevOps engineer and cloud architect. Analyze this Terraform infrastructure change plan and create a concise report for tech managers.

COMMIT MESSAGE: "{commit_message}"

CHANGE STATISTICS:
- Total Resources: {total_resources}
- Creating: {create_count}
- Deleting: {delete_count}
- Updating: {update_count}
- Replacing: {replace_count}
- Critical Resources Affected: {len(critical_changes)}

DETAILED CHANGES:
{change_summary}

CRITICAL RESOURCES:
{chr(10).join([f"- {r.get('address', 'unknown')} ({r.get('type', 'unknown')})" for r in critical_changes[:10]]) if critical_changes else "None"}

Generate a 100-word report for tech managers covering:
1. What changes are happening (focus on the terraform plan changes)
2. Current system state vs proposed state
3. System impact (downtime, performance, security)
4. Safety assessment and recommendations

Respond in this exact format:
SUMMARY: [25 words - executive summary of the change]
CURRENT_STATE: [20 words - describe current infrastructure state]
PROPOSED_CHANGES: [25 words - what will change]
SYSTEM_IMPACT: [20 words - performance, availability, security impact]
RISK_ASSESSMENT: [15 words - key risks]
RECOMMENDATIONS: [15 words - specific recommendations]
SAFETY_RATING: [ONE of: SAFE|CAUTION|RISKY|DANGEROUS]
CONFIDENCE: [0-100]
"""

        # Get LLM response
        if self.llm_provider == "ollama":
            response_text = self._call_llm_raw(prompt, model="llama3")
        elif self.llm_provider == "openai":
            response_text = self._call_openai_raw(prompt)
        elif self.llm_provider == "lmstudio":
            response_text = self._call_lmstudio_raw(prompt)
        else:
            raise NotImplementedError(f"LLM provider '{self.llm_provider}' not implemented")

        return self._parse_impact_report_response(response_text)

    def _call_llm_raw(self, prompt: str, model: str) -> str:
        """Call Ollama LLM and return raw response."""
        try:
            import ollama
            response = ollama.generate(model=model, prompt=prompt)
            return response.get("response", "")
        except ImportError as e:
            raise NotImplementedError("ollama package not installed") from e

    def _call_openai_raw(self, prompt: str) -> str:
        """Call OpenAI API and return raw response."""
        try:
            import openai
            client = openai.OpenAI()
            response = client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[
                    {"role": "system", "content": "You are a cloud infrastructure expert analyzing Terraform changes."},
                    {"role": "user", "content": prompt},
                ],
                temperature=0.7,
                max_tokens=500,
            )
            content = response.choices[0].message.content
            return content if content else ""
        except ImportError as e:
            raise NotImplementedError("openai package not installed") from e

    def _call_lmstudio_raw(self, prompt: str) -> str:
        """Call LMStudio and return raw response."""
        try:
            import openai
            client = openai.OpenAI(
                base_url="http://localhost:1234/v1",
                api_key="not-needed"
            )
            response = client.chat.completions.create(
                model="qwen2.5-coder-7b-instruct",
                messages=[
                    {"role": "system", "content": "You are a cloud infrastructure expert analyzing Terraform changes."},
                    {"role": "user", "content": prompt},
                ],
                temperature=0.7,
                max_tokens=500,
            )
            content = response.choices[0].message.content
            return content if content else ""
        except ImportError as e:
            raise NotImplementedError("openai package not installed") from e

    def _parse_impact_report_response(self, response: str) -> ChangeImpactReport:
        """Parse LLM response into ChangeImpactReport."""
        lines = response.strip().split("\n")
        
        data = {
            "summary": "Infrastructure changes detected.",
            "current_state": "Current infrastructure state.",
            "proposed_changes": "Proposed infrastructure changes.",
            "system_impact": "System impact pending analysis.",
            "risk_assessment": "Risk assessment pending.",
            "recommendations": "Review changes before applying.",
            "safety_rating": "CAUTION",
            "confidence_score": 50.0,
        }
        
        for line in lines:
            if line.startswith("SUMMARY:"):
                data["summary"] = line.split(":", 1)[1].strip()
            elif line.startswith("CURRENT_STATE:"):
                data["current_state"] = line.split(":", 1)[1].strip()
            elif line.startswith("PROPOSED_CHANGES:"):
                data["proposed_changes"] = line.split(":", 1)[1].strip()
            elif line.startswith("SYSTEM_IMPACT:"):
                data["system_impact"] = line.split(":", 1)[1].strip()
            elif line.startswith("RISK_ASSESSMENT:"):
                data["risk_assessment"] = line.split(":", 1)[1].strip()
            elif line.startswith("RECOMMENDATIONS:"):
                data["recommendations"] = line.split(":", 1)[1].strip()
            elif line.startswith("SAFETY_RATING:"):
                data["safety_rating"] = line.split(":", 1)[1].strip().upper()
            elif line.startswith("CONFIDENCE:"):
                try:
                    data["confidence_score"] = int(line.split(":", 1)[1].strip()) / 100.0
                except (ValueError, IndexError):
                    pass
        
        return ChangeImpactReport(**data)


def run_phase4_intent_validation(
    commit_message: str,
    resource_changes: list[dict[str, Any]],
    use_llm: bool = False,
    llm_provider: Optional[str] = None,
    generate_report: bool = False,
) -> IntentValidationResult:
    """Run Phase 4 intent validation.

    Args:
        commit_message: Git commit message.
        resource_changes: List of resource changes.
        use_llm: Whether to use LLM for validation.
        llm_provider: LLM provider name.
        generate_report: Whether to generate a detailed impact report.

    Returns:
        IntentValidationResult with optional report.
    """
    validator = IntentValidator(use_llm=use_llm, llm_provider=llm_provider)
    return validator.validate(commit_message, resource_changes, generate_report=generate_report)
